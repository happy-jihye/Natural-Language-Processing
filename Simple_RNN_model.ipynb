{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_RNN_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCNjUypjfE6D79NtSEjQJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/happy-jihye/Natural-Language-Processing/blob/main/Simple_RNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26nADTPFDL6F"
      },
      "source": [
        "# Simple RNN model\r\n",
        "\r\n",
        "- Pytorch / TorchText\r\n",
        "- RNN network를 사용한 간단한 Sentiment Analysis 예제\r\n",
        "> - 2021/03/21 Happy-jihye\r\n",
        "> - **Reference** : [pytorch-sentiment-analysis/1 - Simple Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0jFZX0dEmMB"
      },
      "source": [
        "## 1. Preparing Data\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFd2lRS-G89C"
      },
      "source": [
        "#### 1) Text/Label\r\n",
        "- **[spaCy](https://spacy.io/)** : nlp를 쉽게 할 수 있도록 도와주는 파이썬 패키지로, tokenizaing, parsing, pos tagging 등을 지원\r\n",
        "- **[Field](https://pytorch.org/text/_modules/torchtext/data/field.html)** 에 정의된 내용을 기반으로 영어를 token화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0cr1n7oE-pM"
      },
      "source": [
        "!pip install -U torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cArSLZmKEydH"
      },
      "source": [
        "%%capture\r\n",
        "!python -m spacy download en"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn9QsVChE48-"
      },
      "source": [
        "import torch\r\n",
        "from torchtext.legacy import data\r\n",
        "# torchtext.legacy : https://stackoverflow.com/questions/66516388/attributeerror-module-torchtext-data-has-no-attribute-field\r\n",
        "\r\n",
        "TEXT = data.Field(tokenize = 'spacy',\r\n",
        "                  tokenizer_language = 'en')\r\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R9bRwoyIia9"
      },
      "source": [
        "#### 2) IMDb Dataset\r\n",
        "- 5만개의 영화 리뷰로 구성된 IMDB dataset을 다운로드 받은 후, 이전에 정의한 Field를 사용해서 데이터를 처리(TEXT, LABEL)\r\n",
        "- torchtext.datasets 의 [IMDB](https://pytorch.org/text/stable/datasets.html#imdb) 객체로 train data와 test data을 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PadXXIFIKXg_"
      },
      "source": [
        "from torchtext.legacy import datasets\r\n",
        "\r\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXkUEhDjK-zn",
        "outputId": "20bee949-da75-4902-bf10-879d9b10b239"
      },
      "source": [
        "print(f'training examples 수 : {len(train_data)}')\r\n",
        "print(f'testing examples 수 : {len(test_data)}')\r\n",
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples 수 : 25000\n",
            "testing examples 수 : 25000\n",
            "{'text': ['This', 'is', 'a', 'neat', 'little', 'crime', 'drama', 'which', 'packs', 'a', 'lot', 'into', 'its', '65', 'minute', 'running', 'time', '.', 'It', 'has', 'all', 'the', 'right', 'ingredients', '-', 'a', 'mystery', 'corpse', ',', 'a', 'weary', 'middle', '-', 'aged', 'cop', 'Corrigan', '(', 'Walter', 'Kinsella', ')', 'and', 'his', 'rookie', 'sidekick', 'Tobin', '(', 'John', 'Miles', ')', ',', 'a', 'shadowy', 'killer', 'on', 'the', 'loose', 'and', 'even', 'love', 'interest', 'for', 'the', 'Tobin', 'in', 'the', 'shape', 'of', 'a', 'female', 'botanist', 'Mary', '(', 'Patricia', 'Wright', ')', 'who', 'helps', 'solve', 'the', 'crime', '.', 'There', \"'s\", 'also', 'a', 'terrific', 'shoot', '-', 'out', 'finale', 'which', 'takes', 'place', 'in', 'a', 'stone', 'cutters', 'yard.<br', '/><br', '/>Watch', 'out', 'for', 'a', 'terrific', 'goof', 'near', 'the', 'start', 'of', 'this', 'movie', 'where', 'Lt', '.', 'Corrigan', 'refers', 'to', 'the', 'dead', 'woman', 'as', \"'\", 'Tatooed', 'Tilly', \"'\", 'BEFORE', 'the', 'coroner', 'reveals', 'that', 'she', 'had', 'a', 'tattoo', '(', 'confusing', 'huh', '?', ')', '.', 'Also', 'later', 'when', 'Tobin', 'is', 'chasing', 'the', 'killer', 'across', 'the', 'back', 'yards', 'he', 'is', 'suddenly', 'shown', 'going', 'in', 'the', 'wrong', 'direction', 'at', 'one', 'point', '-', 'no', 'wonder', 'he', 'did', \"n't\", 'catch', 'him', '!'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YULs86qbL4L9"
      },
      "source": [
        "- IMDb dataset은 train/test data만 있고, validation set이 없음\r\n",
        "\r\n",
        "  따라서 train dataset을 split해서 validation dataset 을 만들기!!\r\n",
        "  (split 함수의 default split_ratio = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9-dgz-PMQp6"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK61YhDIMgPm",
        "outputId": "04472c21-4e57-4052-ede8-7e28be852e69"
      },
      "source": [
        "print(f'training examples 수 : {len(train_data)}')\r\n",
        "print(f'validations examples 수 : {len(valid_data)}')\r\n",
        "print(f'testing examples 수 : {len(test_data)}')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples 수 : 17500\n",
            "validations examples 수 : 7500\n",
            "testing examples 수 : 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMQDknQkNKGY"
      },
      "source": [
        "#### 3) Build Vocabulary\r\n",
        "- one-hot encoding 방식을 사용해서 단어들을 indexing 하기\r\n",
        "- training dataset에 있는 단어들은 10만개가 넘는데, 이 모든 단어들에 대해 indexing을 하면 one-hot vector의 dimension이 10만개가 됨.. 즉, 너무나도 차원이 크기 때문에 연산하기 안좋을 수도 있음\r\n",
        "  - 따라서 자주 나오는 단어들만 가지고 훈련을 하기! 이 예제에서는 **25,000 words**를 사용\r\n",
        "  - ex) \"This film is great and I love it\" 라는 문장에서 \"love\"라는 단어가 vocabulary에 없다면, \"This film is great and I <unk> it\"로 문장을 학습시킬 것\r\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8r09NpIPPXH"
      },
      "source": [
        "MAX_VOCAB_SIZE = "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}